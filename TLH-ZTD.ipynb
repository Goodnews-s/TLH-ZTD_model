{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7713482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score\n",
    "import math\n",
    "import pandas as pd\n",
    "from numpy import *\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dec054",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lssvm#\n",
    "def predict(alphas,b,dataMat,testVec):\n",
    "    Kx = kernelTrans(dataMat,testVec,kTup)   \n",
    "    predict_value =  Kx.T * alphas + b\n",
    "    return predict_value'\n",
    "\n",
    "C = 0.6\n",
    "k1 = 0.3\n",
    "kernel = 'rbf'\n",
    "kTup = (kernel,k1)\n",
    "\n",
    "\n",
    "xlsbpath=r\"/\"\n",
    "os.chdir(xlsbpath) \n",
    "filelist= os.listdir(xlsbpath)\n",
    "\n",
    "def kernelTrans(X,A,kTup):\n",
    "    X = mat(X)\n",
    "    m,n = shape(X)\n",
    "    K = mat(zeros((m,1)))\n",
    "    if kTup[0] == 'lin':\n",
    "        K = X * A.T\n",
    "    elif kTup[0] == 'rbf':\n",
    "        for j in range(m):\n",
    "            deltaRow = X[j,:] - A\n",
    "            K[j] = deltaRow * deltaRow.T\n",
    "        K = exp(K/(-1 * kTup[1] ** 2))\n",
    "    else: raise NameError('Houston We Have a Problem -- That Kernel is not recognized')\n",
    "    return K   \n",
    "class optStruct:\n",
    "    def __init__(self,dataMatIn,classLabels,C,kTup):\n",
    "        self.X = dataMatIn\n",
    "        self.labelMat = classLabels\n",
    "        self.C = C\n",
    "        self.m = shape(dataMatIn)[0]\n",
    "        self.alphas = mat(zeros((self.m,1)))\n",
    "        self.b = 0\n",
    "        self.K = mat(zeros((self.m,self.m)))  \n",
    "        for i in range(self.m):\n",
    "            self.K[:,i] = kernelTrans(self.X, self.X[i,:], kTup)\n",
    "def leastSquares(dataMatIn,classLabels,C,kTup):\n",
    "    oS = optStruct(mat(dataMatIn),mat(classLabels).transpose(),C,kTup)\n",
    "\n",
    "    unit = mat(ones((oS.m,1)))  #[1,1,...,1].T\n",
    "    I = eye(oS.m)\n",
    "    zero = mat(zeros((1,1)))\n",
    "    upmat = hstack((zero,unit.T))\n",
    "    downmat = hstack((unit,oS.K + I/float(C)))\n",
    "\n",
    "    completemat = vstack((upmat,downmat))  \n",
    "    rightmat = vstack((zero,oS.labelMat))   \n",
    "    b_alpha = completemat.I * rightmat\n",
    "    oS.b = b_alpha[0,0]\n",
    "    for i in range(oS.m):\n",
    "        oS.alphas[i,0] = b_alpha[i+1,0]\n",
    "    return oS.alphas,oS.b,oS.K\n",
    "\n",
    "def predict(alphas,b,dataMat,testVec):\n",
    "    Kx = kernelTrans(dataMat,testVec,kTup)   \n",
    "    predict_value =  Kx.T * alphas + b\n",
    "    return predict_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de2c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(filelist)):\n",
    "    time1=time.time()\n",
    "    df=pd.read_excel(\"/\"+filelist[k])\n",
    "    dataMat=[]\n",
    "    labelMat=[]\n",
    "    for i in range(len(df)):\n",
    "        dataMat.append([df['lon'][i],df['lat'][i],df['hig'][i],int(df['time_'][i].split(' ')[0].split('/')[0]),\n",
    "                        int(df['time_'][i].split(' ')[0].split('/')[1]),int(df['time_'][i].split(' ')[0].split('/')[2]),\n",
    "                        int(df['time_'][i].split(' ')[1].split(':')[0]),df['hgpt_ztd'][i]])\n",
    "        labelMat.append(df['ztd'][i]-df['hgpt_ztd'][i])\n",
    "    alphas,b,K = leastSquares(dataMat,labelMat,C,kTup)\n",
    "    pred_=[]\n",
    "    for j in range(len(df)):\n",
    "        pred=predict(alphas,b,dataMat,dataMat[j])\n",
    "        pred_.append(pred)\n",
    "#     pred_L.append(pred_)\n",
    "    pred_L=[]\n",
    "    for i in range(len(pred_)):\n",
    "        pred_L.append(pred_[i].tolist()[0][0])\n",
    "    time2=time.time()\n",
    "    df.insert(df.shape[1],'diff_v',pred_L)\n",
    "    Tem=[]\n",
    "    for z in range(len(df)):\n",
    "        tem=df['tcn_ztd'][z]+pred_L[z]\n",
    "        Tem.append(tem)\n",
    "    df.insert(df.shape[1],'tcn_lssvm_ztd',Tem)\n",
    "    df.to_excel(\"/\"+filelist[k],index=None)\n",
    "    print(filelist[k]+\"time:\"+str(time2-time1)+\"sï¼Œrest:\"+str(34-k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e7eac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Flatten, LeakyReLU, Dropout, Input, BatchNormalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "import math\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "pred_L=[]\n",
    "for k in range(len(filelist)):\n",
    "    try:\n",
    "        df=pd.read_excel(\"/\"+filelist[k])\n",
    "        df=df.drop(['gpt3_ztd','tcn_ztd','tcn_lssvm_ztd'],axis=1)\n",
    "        hgpt_diff=[]\n",
    "        for m in range(len(df)):\n",
    "            hgpt_diff.append(df['hgpt_ztd'][m]+df['diff_v'][m])\n",
    "        df.insert(df.shape[1],'hgpt_diff',hgpt_diff)\n",
    "        df=df.drop(['hgpt_ztd','diff_v'],axis=1)\n",
    "        a=0 \n",
    "        b=0\n",
    "        c=0 \n",
    "        d=0 \n",
    "        e=0\n",
    "        for j in range(len(df['time_'])): \n",
    "            if df['time_'][j][0:4]=='2018':\n",
    "                a+=1\n",
    "            elif df['time_'][j][0:4]=='2019':\n",
    "                b+=1\n",
    "            elif df['time_'][j][0:4]=='2020':\n",
    "                c+=1\n",
    "            elif df['time_'][j][0:4]=='2021':\n",
    "                d+=1\n",
    "            elif df['time_'][j][0:4]=='2022':\n",
    "                e+=1\n",
    "        df1=df[0:a]\n",
    "        df2=df[a:a+b]\n",
    "        df3=df[a+b:a+b+c]\n",
    "        df4=df[a+b+c:a+b+c+d]\n",
    "        df5=df[a+b+c+d:a+b+c+d+e]\n",
    "        time=[df1,df2,df3,df4,df5]\n",
    "        kernel_size = 2\n",
    "        dropout = 0.2\n",
    "        pred_=[]\n",
    "        for i in range(len(time)):\n",
    "            X = time[i].drop([\"ztd\",\"time_\"], axis = 1)\n",
    "            y = time[i][\"ztd\"]\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "            X_train = np.expand_dims(X_train, axis=2)\n",
    "            X_test = np.expand_dims(X_test, axis=2)\n",
    "            model = Sequential()\n",
    "            model.add(Conv1D(32, kernel_size, padding = \"causal\", input_shape = X_train.shape[1:]))\n",
    "            model.add(LeakyReLU(alpha = 0.1))\n",
    "            model.add(BatchNormalization())\n",
    "\n",
    "            model.add(Conv1D(64, kernel_size, padding = \"causal\",  dilation_rate = 2))\n",
    "            model.add(LeakyReLU(alpha = 0.01))\n",
    "            model.add(BatchNormalization())\n",
    "\n",
    "            model.add(Conv1D(128, kernel_size, padding = \"causal\", activation = \"relu\", dilation_rate = 4))\n",
    "            model.add(BatchNormalization())\n",
    "\n",
    "            model.add(Flatten())\n",
    "\n",
    "            model.add(Dense(128, activation = \"relu\"))\n",
    "            model.add(Dropout(dropout))\n",
    "            model.add(Dense(1, activation = \"relu\"))\n",
    "            filepath='weights.best.hdf5'\n",
    "            checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1,save_best_only=True,mode='min',period=1)\n",
    "            callbacks_list = [checkpoint]\n",
    "            model.compile(loss= \"mean_squared_error\", optimizer= \"adam\")\n",
    "        #     model.fit(x=X_train, y=y_train, batch_size =50, epochs=300, validation_data=(X_test, y_test), verbose=1)\n",
    "            model.fit(x=X_train, y=y_train, batch_size =50,epochs=300,validation_data=(X_test, y_test),verbose=1,callbacks=callbacks_list)\n",
    "            model=load_model('weights.best.hdf5')\n",
    "\n",
    "\n",
    "            X = scaler.transform(X)\n",
    "            X = np.expand_dims(X, axis=2)\n",
    "            pred=model.predict(X)\n",
    "            pred_.append(pred)\n",
    "        pred_L.append(pred_)\n",
    "        print(k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k,k)\n",
    "    except:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9393e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
